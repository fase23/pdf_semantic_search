{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major iron ore producers Vale, BHP and Rio Tinto have reported in-line to stronger  \n",
      "than expected production numbers this week. Majors seem on track to deliver  \n",
      "annual production guidance with Vale potentially adding >10mt shipments above  \n",
      "production due to an inventory build up in 3Q22. This comes despite ongoing China  \n",
      "demand weakness and limited room for signiﬁcant demand recover going into the  \n",
      "winter.  \n",
      "On the demand side, the GS China property team (property directly consumes 20%  \n",
      "of steel in the country and 60% indirectly) downgraded housing starts expectations  \n",
      "for 2023 to -20%. And this would follow a sharp 35% decline in 2022. Investors we  \n",
      "spoke with this week were largely surprised by the ongoing China housing market  \n",
      "weakness going into 2023. GS China property numbers are supported by elevated  \n",
      "liquidity pressure and focus on project completion (especially for private players),  \n",
      "weak land bank, rising secondary market supply, still elevated vacancy and others.  \n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# Open the PDF file\n",
    "with open('data/sample1.pdf', 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    \n",
    "    # Extract text from all pages\n",
    "    full_text = \"\"\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        page = reader.pages[page_num]\n",
    "        full_text += page.extract_text()\n",
    "\n",
    "# Optional: Print the first 500 characters to verify extraction\n",
    "print(full_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer Packaged Goods Practice\n",
      "Consumers are in fact buying\n",
      "sustainable goods: Highlights\n",
      "from new research\n",
      "Which sustainability attributes matter most at the point of sale? Do consumers\n",
      "pay attention to sustainability claims? A new report from McKinsey and NielsenIQ\n",
      "provides answers.\n",
      "May 2023“Consumers are shifting their spending toward Monica Toriello: Steve Noble is a senior partner\n",
      "products with ESG-related claims.” That’s one of based in McKinsey’s Minneapolis office. He coleads\n",
      "the overarching insights revealed in a recent study McKinsey’s work in retail transformation globally.\n",
      "conducted by McKinsey and NielsenIQ. The study Steve was on this podcast about two and a half years\n",
      "demonstrated that, in many categories, there’s a ago, and when asked about how his shopping habits\n",
      "clear and substantive correlation between consumer had changed during COVID-19, Steve, you said,\n",
      "spending and sustainability-related claims on “I don’t buy pants anymore; I just buy a lot of wine.”\n",
      "product p\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "# Open the PDF and extract text\n",
    "with pdfplumber.open('data/sample0.pdf') as pdf:\n",
    "    full_text = \"\"\n",
    "    for page in pdf.pages:\n",
    "        full_text += page.extract_text()\n",
    "\n",
    "# Optional: Print the first 500 characters to verify extraction\n",
    "print(full_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Consumer, Coordinates: (73.6772, 146.19720000000007, 117.35249999999999, 155.69720000000007)\n",
      "Text: Packaged, Coordinates: (119.02449999999999, 146.19720000000007, 160.78365, 155.69720000000007)\n",
      "Text: Goods, Coordinates: (162.45565000000002, 146.19720000000007, 189.92585000000003, 155.69720000000007)\n",
      "Text: Practice, Coordinates: (191.59785000000005, 146.19720000000007, 226.82195000000004, 155.69720000000007)\n",
      "Text: Consumers, Coordinates: (73.6772, 164.66070000000002, 257.6276, 202.66070000000002)\n",
      "Text: are, Coordinates: (266.6184, 164.66070000000002, 318.7164, 202.66070000000002)\n",
      "Text: in, Coordinates: (327.6844, 164.66070000000002, 361.18520000000007, 202.66070000000002)\n",
      "Text: fact, Coordinates: (370.15319999999997, 164.66070000000002, 431.07859999999994, 202.66070000000002)\n",
      "Text: buying, Coordinates: (440.0466, 164.66070000000002, 556.1328, 202.66070000000002)\n",
      "Text: sustainable, Coordinates: (73.6772, 204.66070000000002, 256.65479999999997, 242.66070000000002)\n",
      "Text: goods:, Coordinates: (265.6228, 204.66070000000002, 371.34259999999995, 242.66070000000002)\n",
      "Text: Highlights, Coordinates: (380.3106, 204.66070000000002, 554.8370000000001, 242.66070000000002)\n",
      "Text: from, Coordinates: (73.6772, 244.66060000000004, 153.57979999999998, 282.66060000000004)\n",
      "Text: new, Coordinates: (162.54020000000003, 244.66060000000004, 228.69060000000005, 282.66060000000004)\n",
      "Text: research, Coordinates: (237.66240000000002, 244.66060000000004, 376.7576, 282.66060000000004)\n",
      "Text: Which, Coordinates: (73.6772, 291.2373, 109.7146, 305.2373)\n",
      "Text: sustainability, Coordinates: (112.3606, 291.2373, 187.4412, 305.2373)\n",
      "Text: attributes, Coordinates: (190.0872, 291.2373, 245.59439999999998, 305.2373)\n",
      "Text: matter, Coordinates: (248.24039999999997, 291.2373, 286.2937999999999, 305.2373)\n",
      "Text: most, Coordinates: (288.93979999999993, 291.2373, 317.6285999999999, 305.2373)\n",
      "Text: at, Coordinates: (320.27459999999996, 291.2373, 331.399, 305.2373)\n",
      "Text: the, Coordinates: (334.04499999999996, 291.2373, 352.7518, 305.2373)\n",
      "Text: point, Coordinates: (355.39779999999996, 291.2373, 384.57939999999996, 305.2373)\n",
      "Text: of, Coordinates: (387.22540000000004, 291.2373, 398.64520000000005, 305.2373)\n",
      "Text: sale?, Coordinates: (401.2912, 291.2373, 430.62399999999997, 305.2373)\n",
      "Text: Do, Coordinates: (433.27, 291.2373, 449.9356, 305.2373)\n",
      "Text: consumers, Coordinates: (452.5816, 291.2373, 515.6306, 305.2373)\n",
      "Text: pay, Coordinates: (73.6772, 308.2373, 94.1354, 322.2373)\n",
      "Text: attention, Coordinates: (96.7814, 308.2373, 148.3406, 322.2373)\n",
      "Text: to, Coordinates: (150.9866, 308.2373, 162.49599999999998, 322.2373)\n",
      "Text: sustainability, Coordinates: (165.142, 308.2373, 240.22259999999994, 322.2373)\n",
      "Text: claims?, Coordinates: (242.86859999999996, 308.2373, 285.1961999999999, 322.2373)\n",
      "Text: A, Coordinates: (287.84219999999993, 308.2373, 296.36819999999994, 322.2373)\n",
      "Text: new, Coordinates: (299.01419999999996, 308.2373, 322.8015999999999, 322.2373)\n",
      "Text: report, Coordinates: (325.44759999999985, 308.2373, 361.24839999999983, 322.2373)\n",
      "Text: from, Coordinates: (363.8943999999999, 308.2373, 390.5671999999999, 322.2373)\n",
      "Text: McKinsey, Coordinates: (393.2131999999999, 308.2373, 449.34479999999996, 322.2373)\n",
      "Text: and, Coordinates: (451.9907999999999, 308.2373, 473.6445999999999, 322.2373)\n",
      "Text: NielsenIQ, Coordinates: (476.2905999999999, 308.2373, 532.3031999999998, 322.2373)\n",
      "Text: provides, Coordinates: (73.6772, 325.2373, 122.7416, 339.2373)\n",
      "Text: answers., Coordinates: (125.38760000000002, 325.2373, 176.2272, 339.2373)\n",
      "Text: May, Coordinates: (40.9869, 745.3515, 57.452299999999994, 754.8515)\n",
      "Text: 2023, Coordinates: (59.2478, 745.3515, 80.7102, 754.8515)\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "# Open the PDF file\n",
    "with pdfplumber.open('data/sample0.pdf') as pdf:\n",
    "    first_page = pdf.pages[0]  # Get the first page\n",
    "\n",
    "    # Extract detailed information, including text and its location\n",
    "    words_with_coordinates = first_page.extract_words()\n",
    "\n",
    "# Output the text with their bounding boxes (coordinates)\n",
    "for word in words_with_coordinates:\n",
    "    print(f\"Text: {word['text']}, Coordinates: {word['x0'], word['top'], word['x1'], word['bottom']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\User/nltk_data'\n    - 'c:\\\\Users\\\\User\\\\Documents\\\\prove varie\\\\self_made_semantic_search\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\User\\\\Documents\\\\prove varie\\\\self_made_semantic_search\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\User\\\\Documents\\\\prove varie\\\\self_made_semantic_search\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Split the text into sentences\u001b[39;00m\n\u001b[0;32m      4\u001b[0m full_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm a good person because I like nature. I love watching flowers, they bring me joy. What do yo like insetad?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Chunk by 5 sentences at a time\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchunk_by_sentences\u001b[39m(sentences, chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\prove varie\\self_made_semantic_search\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\prove varie\\self_made_semantic_search\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\prove varie\\self_made_semantic_search\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\prove varie\\self_made_semantic_search\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\prove varie\\self_made_semantic_search\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\User/nltk_data'\n    - 'c:\\\\Users\\\\User\\\\Documents\\\\prove varie\\\\self_made_semantic_search\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\User\\\\Documents\\\\prove varie\\\\self_made_semantic_search\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\User\\\\Documents\\\\prove varie\\\\self_made_semantic_search\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Split the text into sentences\n",
    "full_text = \"I'm a good person because I like nature. I love watching flowers, they bring me joy. What do yo like insetad?\"\n",
    "sentences = sent_tokenize(full_text)\n",
    "\n",
    "# Chunk by 5 sentences at a time\n",
    "def chunk_by_sentences(sentences, chunk_size=2):\n",
    "    for i in range(0, len(sentences), chunk_size):\n",
    "        yield ' '.join(sentences[i:i + chunk_size])\n",
    "\n",
    "# Create the sentence chunks\n",
    "sentence_chunks = list(chunk_by_sentences(sentences, chunk_size=5))\n",
    "\n",
    "# Output the first sentence chunk\n",
    "print(sentence_chunks[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May 2023Consumer Packaged Goods Practice Consumers are in fact buying sustainable goods: Highlights from new research Which sustainability attributes matter most at the point of sale? Do consumers pay attention to sustainability claims? A new report from McKinsey and NielsenIQ provides answers.“Consumers are shifting their spending toward products with ESG-related claims.” That’s one of the overarching insights revealed in a recent study conducted by McKinsey and NielsenIQ. The study demonstrated that, in many categories, there’s a clear and substantive correlation between consumer spending and sustainability-related claims on product packaging. In this episode of the McKinsey on Consumer and Retail podcast,\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import pandas as pd\n",
    "\n",
    "# Open the PDF file\n",
    "with open('data/sample0.pdf', 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    \n",
    "    # Extract text from all pages\n",
    "    full_text = \"\"\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        page = reader.pages[page_num]\n",
    "        full_text += page.extract_text()\n",
    "\n",
    "\n",
    "# Now chunk the text (you can chunk by paragraphs, sentences, or a specific number of words)\n",
    "def chunk_text(text, chunk_size=100):  # chunk_size is in number of words\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        yield ' '.join(words[i:i + chunk_size])\n",
    "\n",
    "# Example: Chunk text into blocks of 100 words\n",
    "chunks = list(chunk_text(full_text, chunk_size=100))\n",
    "\n",
    "# Output: Print the first chunk to verify\n",
    "print(chunks[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>page</th>\n",
       "      <th>doc_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>May 2023Consumer Packaged Goods Practice\\nCons...</td>\n",
       "      <td>1</td>\n",
       "      <td>nome documento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Consumers are shifting their spending  toward...</td>\n",
       "      <td>2</td>\n",
       "      <td>nome documento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a gap between people’s sentiment and stated in...</td>\n",
       "      <td>3</td>\n",
       "      <td>nome documento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Steve Noble:  What’s interesting is when you d...</td>\n",
       "      <td>4</td>\n",
       "      <td>nome documento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and really educated to people who are just try...</td>\n",
       "      <td>5</td>\n",
       "      <td>nome documento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Steve Noble:  Monica, your question—“Are compa...</td>\n",
       "      <td>6</td>\n",
       "      <td>nome documento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>as opposed to being a smaller logo on the back...</td>\n",
       "      <td>7</td>\n",
       "      <td>nome documento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>possible on the packaging, the product, and th...</td>\n",
       "      <td>8</td>\n",
       "      <td>nome documento</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  page        doc_name\n",
       "0  May 2023Consumer Packaged Goods Practice\\nCons...     1  nome documento\n",
       "1  “Consumers are shifting their spending  toward...     2  nome documento\n",
       "2  a gap between people’s sentiment and stated in...     3  nome documento\n",
       "3  Steve Noble:  What’s interesting is when you d...     4  nome documento\n",
       "4  and really educated to people who are just try...     5  nome documento\n",
       "5  Steve Noble:  Monica, your question—“Are compa...     6  nome documento\n",
       "6  as opposed to being a smaller logo on the back...     7  nome documento\n",
       "7  possible on the packaging, the product, and th...     8  nome documento"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['text','page','doc_name'])\n",
    "\n",
    "# Open the PDF file\n",
    "with open('data/sample0.pdf', 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        page = reader.pages[page_num]\n",
    "        text = page.extract_text()\n",
    "        df.loc[page_num] = [text,page_num+1, 'nome documento']\n",
    "\n",
    "\n",
    "df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
