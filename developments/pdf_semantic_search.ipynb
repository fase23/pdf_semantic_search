{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\prove varie\\self_made_semantic_search\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>page</th>\n",
       "      <th>doc_name</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a gap between people’s sentiment and stated in...</td>\n",
       "      <td>3</td>\n",
       "      <td>nome documento</td>\n",
       "      <td>0.563038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>as opposed to being a smaller logo on the back...</td>\n",
       "      <td>7</td>\n",
       "      <td>nome documento</td>\n",
       "      <td>0.549852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>May 2023Consumer Packaged Goods Practice\\nCons...</td>\n",
       "      <td>1</td>\n",
       "      <td>nome documento</td>\n",
       "      <td>0.522379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>possible on the packaging, the product, and th...</td>\n",
       "      <td>8</td>\n",
       "      <td>nome documento</td>\n",
       "      <td>0.517816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Consumers are shifting their spending  toward...</td>\n",
       "      <td>2</td>\n",
       "      <td>nome documento</td>\n",
       "      <td>0.447652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Steve Noble:  Monica, your question—“Are compa...</td>\n",
       "      <td>6</td>\n",
       "      <td>nome documento</td>\n",
       "      <td>0.438222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and really educated to people who are just try...</td>\n",
       "      <td>5</td>\n",
       "      <td>nome documento</td>\n",
       "      <td>0.317439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Steve Noble:  What’s interesting is when you d...</td>\n",
       "      <td>4</td>\n",
       "      <td>nome documento</td>\n",
       "      <td>0.312207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  page        doc_name  \\\n",
       "2  a gap between people’s sentiment and stated in...     3  nome documento   \n",
       "6  as opposed to being a smaller logo on the back...     7  nome documento   \n",
       "0  May 2023Consumer Packaged Goods Practice\\nCons...     1  nome documento   \n",
       "7  possible on the packaging, the product, and th...     8  nome documento   \n",
       "1  “Consumers are shifting their spending  toward...     2  nome documento   \n",
       "5  Steve Noble:  Monica, your question—“Are compa...     6  nome documento   \n",
       "4  and really educated to people who are just try...     5  nome documento   \n",
       "3  Steve Noble:  What’s interesting is when you d...     4  nome documento   \n",
       "\n",
       "   similarity  \n",
       "2    0.563038  \n",
       "6    0.549852  \n",
       "0    0.522379  \n",
       "7    0.517816  \n",
       "1    0.447652  \n",
       "5    0.438222  \n",
       "4    0.317439  \n",
       "3    0.312207  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# read pdf and populate the df\n",
    "df = pd.DataFrame(columns=['text','page','doc_name'])\n",
    "with open('data/sample0.pdf', 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        page = reader.pages[page_num]\n",
    "        text = page.extract_text()\n",
    "        df.loc[page_num] = [text,page_num+1, 'nome documento']\n",
    "\n",
    "\n",
    "# Load the semantich model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "query = \"the impact of sustainability on the company\"\n",
    "\n",
    "# embedding\n",
    "embedding = model.encode(df['text'])\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "# Compute the cosine similarity\n",
    "similarities = model.similarity(query_embedding, embedding)[0]\n",
    "\n",
    "df['similarity'] = similarities\n",
    "\n",
    "df\n",
    "df.sort_values(by='similarity', ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\prove varie\\self_made_semantic_search\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\User\\Documents\\prove varie\\self_made_semantic_search\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\prove varie\\self_made_semantic_search\\.venv\\Lib\\site-packages\\sentence_transformers\\util.py:44: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  a = torch.tensor(a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>page</th>\n",
       "      <th>doc_name</th>\n",
       "      <th>embedding</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and really educated to people who are just try...</td>\n",
       "      <td>5</td>\n",
       "      <td>nome documento</td>\n",
       "      <td>[0.013642773, 0.056959312, 0.10014118, 0.06927...</td>\n",
       "      <td>0.472103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a gap between people’s sentiment and stated in...</td>\n",
       "      <td>3</td>\n",
       "      <td>nome documento</td>\n",
       "      <td>[-0.0046468237, 0.0037014936, 0.026363492, 0.0...</td>\n",
       "      <td>0.400419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Steve Noble:  Monica, your question—“Are compa...</td>\n",
       "      <td>6</td>\n",
       "      <td>nome documento</td>\n",
       "      <td>[0.00085351267, 0.0010896728, 0.09292241, -0.0...</td>\n",
       "      <td>0.331254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>as opposed to being a smaller logo on the back...</td>\n",
       "      <td>7</td>\n",
       "      <td>nome documento</td>\n",
       "      <td>[0.07595144, -0.021715835, 0.027349357, -0.003...</td>\n",
       "      <td>0.327104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Steve Noble:  What’s interesting is when you d...</td>\n",
       "      <td>4</td>\n",
       "      <td>nome documento</td>\n",
       "      <td>[-0.00927053, 0.011570626, -0.00479435, -0.002...</td>\n",
       "      <td>0.320657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>possible on the packaging, the product, and th...</td>\n",
       "      <td>8</td>\n",
       "      <td>nome documento</td>\n",
       "      <td>[-0.045609158, 0.01657531, 0.028220344, 0.0768...</td>\n",
       "      <td>0.312604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Consumers are shifting their spending  toward...</td>\n",
       "      <td>2</td>\n",
       "      <td>nome documento</td>\n",
       "      <td>[-0.033085335, -0.0058989744, 0.006865788, -0....</td>\n",
       "      <td>0.311284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>May 2023Consumer Packaged Goods Practice\\nCons...</td>\n",
       "      <td>1</td>\n",
       "      <td>nome documento</td>\n",
       "      <td>[-0.045357022, 0.022532715, 0.020105349, 0.001...</td>\n",
       "      <td>0.300178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  page        doc_name  \\\n",
       "4  and really educated to people who are just try...     5  nome documento   \n",
       "2  a gap between people’s sentiment and stated in...     3  nome documento   \n",
       "5  Steve Noble:  Monica, your question—“Are compa...     6  nome documento   \n",
       "6  as opposed to being a smaller logo on the back...     7  nome documento   \n",
       "3  Steve Noble:  What’s interesting is when you d...     4  nome documento   \n",
       "7  possible on the packaging, the product, and th...     8  nome documento   \n",
       "1  “Consumers are shifting their spending  toward...     2  nome documento   \n",
       "0  May 2023Consumer Packaged Goods Practice\\nCons...     1  nome documento   \n",
       "\n",
       "                                           embedding  similarity  \n",
       "4  [0.013642773, 0.056959312, 0.10014118, 0.06927...    0.472103  \n",
       "2  [-0.0046468237, 0.0037014936, 0.026363492, 0.0...    0.400419  \n",
       "5  [0.00085351267, 0.0010896728, 0.09292241, -0.0...    0.331254  \n",
       "6  [0.07595144, -0.021715835, 0.027349357, -0.003...    0.327104  \n",
       "3  [-0.00927053, 0.011570626, -0.00479435, -0.002...    0.320657  \n",
       "7  [-0.045609158, 0.01657531, 0.028220344, 0.0768...    0.312604  \n",
       "1  [-0.033085335, -0.0058989744, 0.006865788, -0....    0.311284  \n",
       "0  [-0.045357022, 0.022532715, 0.020105349, 0.001...    0.300178  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# Load the semantich model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# read pdf and populate the df\n",
    "df = pd.DataFrame(columns=['text','page','doc_name','embedding'])\n",
    "with open('data/sample0.pdf', 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        page = reader.pages[page_num]\n",
    "        text = page.extract_text()\n",
    "        emb_text = model.encode(text)\n",
    "        df.loc[page_num] = [text,page_num+1, 'nome documento', emb_text]\n",
    "\n",
    "\n",
    "\n",
    "query = \"talk about how the climate change affects the companies\"\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "# Compute the cosine similarity\n",
    "similarities = model.similarity(query_embedding, df['embedding'])[0]\n",
    "\n",
    "df['similarity'] = similarities\n",
    "df.sort_values(by='similarity', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
